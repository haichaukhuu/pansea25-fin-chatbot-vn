#!/usr/bin/env python3
"""
Example Usage of AI Models

This script shows practical examples of how to use the AI models
for building a financial literacy chatbot.
"""

import asyncio
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import AI models
from ai_models import ModelFactory, ModelManager, ModelName

async def simple_chat_example():
    """Simple chat example with the Gemini model"""
    print("=== Simple Chat Example (Gemini) ===")
    
    try:
        # Create Gemini model
        gemini = ModelFactory.create_gemini_model()
        
        # Simple question
        question = "Xin ch√†o, t√¥i mu·ªën bi·∫øt v·ªÅ l√£i su·∫•t ti·∫øt ki·ªám ng√¢n h√†ng"
        response = await gemini.generate_response(question)
        
        print(f"User: {question}")
        print(f"Bot: {response.content}")
        print(f"Model: {response.model_used}")
        
    except Exception as e:
        print(f"Error: {e}")

async def gemma_chat_example():
    """Simple chat example with the Gemma model"""
    print("\n=== Simple Chat Example (Gemma) ===")
    
    try:
        # Create Gemma model
        gemma = ModelFactory.create_gemma_model(ModelName.GEMMA_3N_2B)
        
        # Simple question
        question = "Xin ch√†o, t√¥i mu·ªën bi·∫øt v·ªÅ l√£i su·∫•t vay n√¥ng nghi·ªáp"
        response = await gemma.generate_response(question)
        
        print(f"User: {question}")
        print(f"Bot: {response.content}")
        print(f"Model: {response.model_used}")
        print(f"Model Type: {gemma.get_model_type()}")
        
    except Exception as e:
        print(f"Error: {e}")

async def context_aware_chat():
    """Chat with context awareness"""
    print("\n=== Context-Aware Chat Example ===")
    
    try:
        # Create Gemini model for context-aware chat
        gemini = ModelFactory.create_gemini_model()
        
        # User profile and context
        context = {
            "user_profile": {
                "location": "An Giang",
                "farming_type": "nu√¥i t√¥m",
                "experience": "5 nƒÉm"
            },
            "chat_history": [
                {"role": "user", "content": "T√¥i mu·ªën vay ti·ªÅn ƒë·ªÉ m·ªü r·ªông ao nu√¥i t√¥m"},
                {"role": "bot", "content": "T√¥i hi·ªÉu b·∫°n mu·ªën vay ti·ªÅn ƒë·ªÉ m·ªü r·ªông ao nu√¥i t√¥m. B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt th√™m v·ªÅ k·∫ø ho·∫°ch c·ªßa b·∫°n kh√¥ng?"}
            ]
        }
        
        # Follow-up question with context
        question = "V·∫≠y l√£i su·∫•t vay s·∫Ω l√† bao nhi√™u v√† t√¥i c·∫ßn chu·∫©n b·ªã nh·ªØng g√¨?"
        
        response = await gemini.generate_response(question, context=context)
        
        print(f"User Profile: {context['user_profile']}")
        print(f"Chat History: {len(context['chat_history'])} messages")
        print(f"User: {question}")
        print(f"Bot: {response.content}")
        
    except Exception as e:
        print(f"Error: {e}")

async def rag_example():
    """Example of using models with RAG context"""
    print("\n=== RAG Context Example ===")
    
    try:
        # Create models
        gemini = ModelFactory.create_gemini_model()
        embedding_model = ModelFactory.create_embedding_model()
        
        # Simulate RAG context (in real app, this would come from vector search)
        rag_context = """
        Th√¥ng tin v·ªÅ vay n√¥ng nghi·ªáp:
        - L√£i su·∫•t: 6-8% m·ªói nƒÉm
        - H·∫°n m·ª©c: T·ªëi ƒëa 500 tri·ªáu VND
        - Th·ªùi h·∫°n: 1-5 nƒÉm
        - ƒêi·ªÅu ki·ªán: C√≥ ƒë·∫•t canh t√°c, k·∫ø ho·∫°ch s·∫£n xu·∫•t r√µ r√†ng
        - H·ªì s∆° c·∫ßn thi·∫øt: CMND, s·ªï ƒë·ªè, k·∫ø ho·∫°ch s·∫£n xu·∫•t, b√°o c√°o t√†i ch√≠nh
        """
        
        # Generate embedding for the context (for demonstration)
        context_embedding = await embedding_model.generate_embedding(rag_context)
        print(f"Generated embedding for RAG context: {len(context_embedding)} dimensions")
        
        # Use RAG context in response
        context = {"rag_context": rag_context}
        question = "T√¥i mu·ªën vay 200 tri·ªáu ƒë·ªÉ mua m√°y c√†y, c√≥ ƒë∆∞·ª£c kh√¥ng?"
        
        response = await gemini.generate_response(question, context=context)
        
        print(f"RAG Context: {rag_context[:100]}...")
        print(f"User: {question}")
        print(f"Bot: {response.content}")
        
    except Exception as e:
        print(f"Error: {e}")

async def streaming_chat():
    """Example of streaming chat responses"""
    print("\n=== Streaming Chat Example ===")
    
    try:
        # Create Gemini model
        gemini = ModelFactory.create_gemini_model()
        
        question = "H√£y gi·∫£i th√≠ch chi ti·∫øt v·ªÅ c√°c lo·∫°i b·∫£o hi·ªÉm n√¥ng nghi·ªáp v√† l·ª£i √≠ch c·ªßa ch√∫ng"
        
        print(f"User: {question}")
        print("Bot: ", end="", flush=True)
        
        # Stream the response
        async for chunk in gemini.generate_streaming_response(question):
            print(chunk, end="", flush=True)
        
        print()  # New line after streaming
        
    except Exception as e:
        print(f"Error: {e}")

async def model_comparison():
    """Compare different models"""
    print("\n=== Model Comparison Example ===")
    
    try:
        # Create different models
        gemini_pro = ModelFactory.create_gemini_model(ModelName.GEMINI_PRO)
        gemma_2b = ModelFactory.create_gemma_model(ModelName.GEMMA_3N_2B)
        
        question = "Gi·∫£i th√≠ch ng·∫Øn g·ªçn v·ªÅ l√£i su·∫•t k√©p"
        
        print(f"Question: {question}")
        
        # Test Gemini Pro
        print(f"\n--- {gemini_pro.config.name} ---")
        response_gemini = await gemini_pro.generate_response(question)
        print(f"Response: {response_gemini.content[:200]}...")
        
        # Test Gemma 2B
        print(f"\n--- {gemma_2b.config.name} ---")
        response_gemma = await gemma_2b.generate_response(question)
        print(f"Response: {response_gemma.content[:200]}...")
        
    except Exception as e:
        print(f"Error: {e}")

async def model_manager_example():
    """Example using the model manager"""
    print("\n=== Model Manager Example ===")
    
    try:
        # Create model manager
        manager = await ModelManager.create_default_manager()
        
        # Get model status
        status = manager.get_model_status()
        print("Model Status:")
        for name, info in status.items():
            print(f"  {name}: {info['type']} - {'Available' if info['is_available'] else 'Unavailable'}")
        
        # Use unified interface
        question = "T√¥i mu·ªën bi·∫øt v·ªÅ ƒë·∫ßu t∆∞ v√†o c√¥ng ngh·ªá n√¥ng nghi·ªáp"
        
        print(f"\nUser: {question}")
        print("Bot: ", end="", flush=True)
        
        # Generate streaming response through manager
        async for chunk in manager.generate_streaming_response(question):
            print(chunk, end="", flush=True)
        
        print()  # New line after streaming
        
        # Stop health monitoring
        manager.stop_health_monitoring()
        
    except Exception as e:
        print(f"Error: {e}")

async def main():
    """Main function to run all examples"""
    print("üöÄ AI Models Usage Examples")
    print("Make sure you have set GOOGLE_GENAI_API_KEY environment variable")
    
    # Check if API key is available
    api_key = os.getenv("GOOGLE_GENAI_API_KEY")
    if not api_key:
        print("‚ùå GOOGLE_GENAI_API_KEY environment variable not set!")
        print("Please set it with your Google GenAI API key to run the examples.")
        return
    
    print(f"‚úì Found Google GenAI API key: {api_key[:10]}...")
    
    try:
        # Run examples
        await simple_chat_example()
        await gemma_chat_example()
        await context_aware_chat()
        await rag_example()
        await streaming_chat()
        await model_comparison()
        await model_manager_example()
        
        print("\n‚úÖ All examples completed successfully!")
        
    except Exception as e:
        print(f"\n‚ùå Examples failed with error: {e}")

if __name__ == "__main__":
    # Run the async main function
    asyncio.run(main())
