import json
import logging
import boto3
import numpy as np
from typing import Dict, Any, List, Optional, AsyncIterator
from .base_model import BaseModel, ModelResponse, ModelConfig, ModelCapability

logger = logging.getLogger(__name__)

class BedrockEmbeddingModel(BaseModel):
    """AWS Bedrock Titan Text Embeddings model for RAG system"""
    
    def __init__(self, config: ModelConfig = None, region_name: str = None, api_key: str = None):
        if config is None:
            config = ModelConfig(
                name="aws-titan-embed-text-v2",
                model_id="amazon.titan-embed-text-v2:0",
                capabilities=[ModelCapability.MULTILINGUAL],
                max_tokens=8192,  # Titan Text Embeddings max tokens
                temperature=0.0,
                is_primary=True,
                priority=1
            )
        
        super().__init__(config)
        self.model_id = config.model_id
        self.embedding_dim = 1536  # Titan Text Embeddings v2 dimension
        self.region_name = region_name or "us-east-1"
        
        try:
            # Initialize Bedrock client
            self.client = boto3.client(
                service_name='bedrock-runtime',
                region_name=self.region_name
            )
            logger.info(f"Initialized AWS Bedrock embedding model: {self.config.name}")
        except Exception as e:
            logger.error(f"Failed to initialize AWS Bedrock embedding model {self.config.name}: {e}")
            self.is_available = False
    
    async def generate_response(
        self, 
        prompt: str, 
        context: Dict[str, Any] = None,
        system_prompt: str = None,
        tools: List[Dict] = None
    ) -> ModelResponse:
        """Embedding models don't generate text responses"""
        raise NotImplementedError("BedrockEmbeddingModel does not support text generation")
    
    async def generate_streaming_response(
        self,
        prompt: str,
        context: Dict[str, Any] = None,
        system_prompt: str = None
    ) -> AsyncIterator[str]:
        """Embedding models don't support streaming text responses"""
        raise NotImplementedError("BedrockEmbeddingModel does not support streaming text generation")
    
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embeddings for text using AWS Bedrock Titan Text Embeddings"""
        try:
            # Prepare request payload
            request_body = {
                "inputText": text
            }
            
            # Make the API call
            response = self.client.invoke_model(
                modelId=self.model_id,
                body=json.dumps(request_body)
            )
            
            # Parse the response
            response_body = json.loads(response.get('body').read())
            embedding = response_body.get('embedding', [])
            
            return embedding
            
        except Exception as e:
            logger.error(f"Error generating embedding with AWS Bedrock: {e}")
            self.mark_error()
            raise
    
    async def embed_documents(self, documents: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple documents"""
        embeddings = []
        for doc in documents:
            embedding = await self.generate_embedding(doc)
            embeddings.append(embedding)
        return embeddings
    
    async def embed_query(self, query: str) -> List[float]:
        """Generate embedding for a query"""
        return await self.generate_embedding(query)
    
    async def embed_passage(self, passage: str) -> List[float]:
        """Generate embedding for a passage"""
        return await self.generate_embedding(passage)
    
    async def health_check(self) -> bool:
        try:
            # Test with a simple Vietnamese text
            test_text = "Xin chÃ o"
            embedding = await self.generate_embedding(test_text)
            
            if len(embedding) == self.embedding_dim:
                self.is_available = True
                self.reset_errors()
                return True
            else:
                self.is_available = False
                return False
                
        except Exception as e:
            logger.error(f"Health check failed for {self.config.name}: {e}")
            self.is_available = False
            return False
    
    def get_embedding_dimension(self) -> int:
        """Get the dimension of embeddings generated by this model"""
        return self.embedding_dim
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get detailed model information"""
        info = super().get_model_info()
        info.update({
            "embedding_dimension": self.embedding_dim,
            "model_id": self.model_id,
            "region": self.region_name
        })
        return info
